{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal of this project is to identify unusual patterns in catalog or potential data manipulation inacuracies\n",
    "\n",
    "## Project Checklist\n",
    "- Frame the project to get the big picture\n",
    "  - Why identify anomalies in the data? \n",
    "    - to validate the reports made from the data and define biases based on the (in)completness of the data \n",
    "    - to identify patterns missed in the reports.\n",
    "- Get the [data](https://docs.google.com/spreadsheets/d/173kXrmgG0K4Q_K0d2GFgADnW3wB66xh9R1IdyjB3eXY/edit?gid=1#gid=1)\n",
    "- Explore the data\n",
    "- Prepare the data to better expose the underlying patterns\n",
    "- Explore models and pick the best one(s)\n",
    "- Present solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install ipywidgets widgetsnbextension jupyter_contrib_nbextensions\n",
    "%pip install pandas numpy matplotlib seaborn scikit-learn streamlit\n",
    "# Enable Jupyter Notebook extensions\n",
    "!jupyter contrib nbextension install --user\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "%pip install tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_url(url: str):\n",
    "    \"\"\"\n",
    "    Checks if the URL is valid.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to check.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the URL is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spreadsheet_to_csv(spreadsheet_url: str):\n",
    "    \"\"\" \n",
    "        This function will convert the google spreadsheet url to the csv url\n",
    "        Parameters:\n",
    "            spreadsheet_url (string): The google spreadsheet url\n",
    "        Returns:\n",
    "            string: The csv url\n",
    "    \"\"\"\n",
    "    return re.sub(r'/edit.*$', '/export?format=csv', spreadsheet_url)\n",
    "\n",
    "def get_the_data(sheet_url:str='https://docs.google.com/spreadsheets/d/173kXrmgG0K4Q_K0d2GFgADnW3wB66xh9R1IdyjB3eXY/edit?gid=1#gid=1'):\n",
    "    \"\"\" \n",
    "        This function will get the data from the google sheet and return it as a pandas dataframe \n",
    "        Parameters:\n",
    "            sheet_url (string): The url of the google sheet\n",
    "        Returns:\n",
    "            pandas dataframe: The data from the google sheet\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_export_url = convert_spreadsheet_to_csv(sheet_url) # Convert the google sheet url to csv url\n",
    "    \n",
    "    if sheet_url == '':\n",
    "        return pd.DataFrame()\n",
    "    try:       \n",
    "        if not is_valid_url(sheet_url):\n",
    "            print(f\"Invalid URL: {sheet_url}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        r = requests.head(csv_export_url, verify=False, allow_redirects=True)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(csv_export_url, verify=False) # Make the request to the url\n",
    "        if response.status_code == 200:\n",
    "            df= pd.read_csv(csv_export_url)\n",
    "        elif response.status_code == 401:\n",
    "            print(f'Authentication issue {csv_export_url} : {response.status_code}') # - {response.text}')\n",
    "            df = pd.DataFrame()\n",
    "        else:\n",
    "            print(f'Failed to retrieve data from {csv_export_url} : {response.status_code}') # - {response.text}')\n",
    "            df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f'Failed to retrieve data from {csv_export_url} : {e}')\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df\n",
    "\n",
    "dataset_base = get_the_data()\n",
    "dataset_base.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(url: str):\n",
    "    \"\"\"\n",
    "        This function will check the schema of the url and returns a correct scheme \n",
    "    \"\"\"\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "\n",
    "    # print(f\"URL: {url}\")\n",
    "    return url\n",
    "\n",
    "def convert_bitly_to_url(bitly_url: str):\n",
    "    \"\"\" \n",
    "        This function will convert the bitly url to the original google sheet url\n",
    "        Parameters:\n",
    "            bitly_url (string): The bitly url\n",
    "        Returns:\n",
    "            string: The original google sheet url\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res:str = requests.get(bitly_url, verify=False, allow_redirects=True).url\n",
    "    except requests.exceptions.SSLError as ssl_error: # The URL is not valid\n",
    "        # print(f\"SSL Error: {ssl_error}\")\n",
    "        res = bitly_url\n",
    "    return res\n",
    "\n",
    "def convert_geni_to_url(geni_url:str):\n",
    "    \"\"\" \n",
    "        This function will convert the geni url to the original google sheet url\n",
    "        Parameters:\n",
    "            geni_url (string): The geni url\n",
    "        Returns:\n",
    "            string: The original google sheet url\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res:str = requests.get(geni_url, verify=False, allow_redirects=True).url\n",
    "    except requests.exceptions.SSLError as ssl_error: # The URL is not valid\n",
    "        print(f\"SSL Error: {ssl_error}\")\n",
    "        res = geni_url\n",
    "    return res\n",
    "   \n",
    "def convert_airtable_to_url(airtable_url: str):\n",
    "    \"\"\" \n",
    "        This function will convert the airtable url to the original google sheet url\n",
    "        Parameters:\n",
    "            airtable_url (string): The airtable url\n",
    "        Returns:\n",
    "            string: The original google sheet url\n",
    "    \"\"\"\n",
    "    res:str = re.sub(r'/tbl([^/]+)/([^/]+)', r'/tbl\\1/\\2/csv', airtable_url)\n",
    "    print(f\"Airtable URL: {airtable_url} - \\nConverted URL: {res}\")\n",
    "    return res\n",
    "        \n",
    "def get_datasets(base_df: pd.DataFrame=dataset_base, target_column: str = 'bitly '):\n",
    "    \"\"\"\n",
    "    This function will get the datasets from the 'bitly' column in the dataframe and return the datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        base_df (pandas dataframe): The dataframe containing the data.\n",
    "        target_column (string): The name of the target column.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries with 'entry' and 'title' as keys.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for _, row in base_df.iterrows():\n",
    "        entry = row[target_column].strip()\n",
    "        title = row['title']\n",
    "        entry = check_schema(entry)\n",
    "\n",
    "        conversion_functions = {\n",
    "            'spreadsheets': convert_spreadsheet_to_csv,\n",
    "            'bit.ly': convert_bitly_to_url,\n",
    "            'BIT.LY': convert_bitly_to_url,\n",
    "            'airtable.com': convert_airtable_to_url,\n",
    "            'geni': convert_geni_to_url,\n",
    "        }\n",
    "\n",
    "        for keyword, func in conversion_functions.items():\n",
    "            if keyword in entry:\n",
    "                entry = func(entry)\n",
    "                break\n",
    "\n",
    "        if 'informationisbeautiful' in entry:\n",
    "            # skip the entry\n",
    "            entry = ''\n",
    "\n",
    "        # print(f\"Getting the dataset from URL: {entry}\")\n",
    "        dataset = get_the_data(entry)\n",
    "\n",
    "        if not dataset.empty:\n",
    "            datasets.append({'entry': dataset, 'title': title})\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# print(f'Base Dataset Shape: {dataset_base.shape}') -> (151, 6)\n",
    "multiple_datasets = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry, dataset in enumerate(multiple_datasets):\n",
    "    print(f\"{entry:3d} Title: {dataset['title']} Shape: {dataset['entry'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data to better expose the underlying patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(datasets:list):\n",
    "    \"\"\"\n",
    "        This function will remove the datasets with the same shapes, leaving a single copy.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            list: The list of datasets without duplicates \n",
    "    \"\"\"\n",
    "    unique_datasets = []\n",
    "    for dataset in datasets:\n",
    "        if dataset.shape not in [data.shape for data in unique_datasets]:\n",
    "            unique_datasets.append(dataset)\n",
    "    return unique_datasets\n",
    "\n",
    "def remove_small_datasets(datasets:list):\n",
    "    \"\"\"\n",
    "        This function will remove the datasets with less than 10 rows.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            list: The list of datasets without small datasets \n",
    "    \"\"\"\n",
    "    return [dataset for dataset in datasets if dataset.shape[0] > 10 & dataset.shape[1] > 2]\n",
    "\n",
    "def handle_missing_values(datasets:list):\n",
    "    \"\"\"\n",
    "        This function will handle the missing values in the datasets.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            list: The list of datasets without missing values \n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        dataset.fillna(method='ffill', inplace=True) \n",
    "    return datasets\n",
    "\n",
    "def find_non_alphanumeric_data(datasets: list):\n",
    "    \"\"\"\n",
    "    This function will list the datasets with columns containing non-alphanumeric data.\n",
    "    \n",
    "    Parameters:\n",
    "        datasets (list): The list of datasets\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of non-alphanumeric characters found in the datasets\n",
    "    \"\"\"\n",
    "    non_alphanumeric_chars = set()\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        for column in dataset.columns:\n",
    "            for index, value in dataset[column].items():\n",
    "                if isinstance(value, str):  # Check if the value is a string\n",
    "                    non_alphanumeric = re.findall(r'[^a-zA-Z0-9]', value)\n",
    "                    if non_alphanumeric:\n",
    "                        non_alphanumeric_chars.update(non_alphanumeric)\n",
    "                        # print(f\"Dataset {idx}, Row {index}, Column '{column}': \\\n",
    "                        #       Non-alphanumeric characters: {non_alphanumeric}\")\n",
    "    return list(non_alphanumeric_chars)\n",
    "\n",
    "def replace_non_alphanumeric_data(datasets: list, non_alphanumeric_chars: list):\n",
    "    \"\"\"\n",
    "    This function will replace non-alphanumeric characters in the datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        datasets (list): The list of datasets\n",
    "        non_alphanumeric_chars (list): The list of non-alphanumeric characters to replace\n",
    "    \n",
    "    Returns:\n",
    "        datasets (list): The list of datasets with non-alphanumeric characters replaced\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        for column in dataset.columns:\n",
    "            for index, value in dataset[column].items():\n",
    "                if isinstance(value, str):  # Check if the value is a string\n",
    "                    for char in non_alphanumeric_chars:\n",
    "                        value = value.replace(char, '')\n",
    "                    dataset.at[index, column] = value\n",
    "    return datasets\n",
    "\n",
    "datasets = [dataset['entry'] for dataset in multiple_datasets]\n",
    "\n",
    "print(f\"Number of datasets: {len(datasets)}\", end='\\t')\n",
    "datasets = remove_duplicates(datasets)\n",
    "datasets = remove_small_datasets(datasets)\n",
    "datasets = handle_missing_values(datasets)\n",
    "\n",
    "non_alphanumeric_chars:list = find_non_alphanumeric_data(datasets)\n",
    "datasets = replace_non_alphanumeric_data(datasets, non_alphanumeric_chars)\n",
    "\n",
    "print(f'Editted number of datasets: {len(datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(datasets:list=datasets):\n",
    "    \"\"\"\n",
    "        This function will get the column names of each dataset in the list of datasets.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            List of list: The list of column names of each dataset\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        categorical_columns, numeric_columns = [],[]\n",
    "\n",
    "        numeric_columns.append(dataset.select_dtypes(include=[np.number]).columns)\n",
    "        categorical_columns.append(dataset.select_dtypes(include=[object]).columns)\n",
    "\n",
    "        print(f\"Numeric Columns: {list(numeric_columns)}\\nCategorical Columns: {list(categorical_columns)}\\n\\n\")\n",
    "    return [list(dataset.columns) for dataset in datasets]\n",
    "\n",
    "def clean_columns(datasets:list=datasets):\n",
    "    \"\"\"\n",
    "        This function will remove the columns with links\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            datasets (list): The list of datasets without columns with links\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        for column in dataset.columns:\n",
    "            if 'http' in column:\n",
    "                dataset.drop(column, axis=1, inplace=True)\n",
    "    return datasets\n",
    "\n",
    "columns = get_column_names(datasets)\n",
    "columns = clean_columns(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(datasets: list):\n",
    "    \"\"\"\n",
    "    This function will plot the data distribution of the datasets.\n",
    "    For each dataset, plot a bunch of subplots for each column.\n",
    "    \n",
    "    Parameters:\n",
    "        datasets (list): The list of datasets.\n",
    "    \"\"\"\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        if idx < 10:\n",
    "            numeric_columns = dataset.select_dtypes(include=['number']).columns\n",
    "            num_columns = len(numeric_columns)\n",
    "\n",
    "            if num_columns == 0:\n",
    "                print(f\"Dataset {idx} has no numeric columns to plot.\")\n",
    "                continue\n",
    "            \n",
    "            num_rows = (num_columns // 4) + (num_columns % 4 > 0)\n",
    "            fig, axs = plt.subplots(num_rows, min(4, num_columns), figsize=(15, 5 * num_rows))\n",
    "            axs = axs.flatten() if num_columns > 1 else [axs]\n",
    "\n",
    "            for col_idx, col_name in enumerate(numeric_columns):\n",
    "                axs[col_idx].plot(dataset[col_name])\n",
    "                axs[col_idx].set_title(f'Dataset-{idx} {dataset.shape}', fontsize=8)\n",
    "                axs[col_idx].set_xlabel(f'{col_name.replace('$', '\\\\$')}', fontsize=8)\n",
    "                axs[col_idx].set_ylabel('Value')\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            plt.show() \n",
    "\n",
    "plot_data_distribution(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical columns to numerical columns\n",
    "from sklearn.preprocessing import LabelEncoder # Encode target labels with value between 0 and n_classes-1.\n",
    "from sklearn.preprocessing import StandardScaler # Removing the mean and scaling to unit variance\n",
    "\n",
    "def convert_categorical_columns(datasets:list=datasets):\n",
    "    \"\"\"\n",
    "        This function will convert the categorical columns to numerical columns.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            datasets (list): The list of datasets with numerical columns\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        categorical_columns = dataset.select_dtypes(include=[object]).columns\n",
    "        for column in categorical_columns:\n",
    "            encoder = LabelEncoder()\n",
    "            dataset[column] = encoder.fit_transform(dataset[column])\n",
    "        \n",
    "        # Replace the $ sign with empty space for values in each column\n",
    "        for col in dataset.select_dtypes(include=['object']):  # Only process string columns\n",
    "            # Replace dollar signs with spaces\n",
    "            dataset[col] = dataset[col].str.replace('$', ' ', regex=False)\n",
    "            cleaned_text = dataset[col].strip('$')  # Remove leading and trailing dollar signs\n",
    "            cleaned_text = cleaned_text.replace('$', '\\\\$')  # Escape dollar signs within the text\n",
    "            dataset[col] = cleaned_text    \n",
    "\n",
    "    return datasets\n",
    "\n",
    "numerically_datasets = convert_categorical_columns(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # Scaling features to a range\n",
    "\n",
    "def scale_datasets(datasets:list=numerically_datasets):\n",
    "    \"\"\"\n",
    "        This function will scale the datasets.\n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        Returns:\n",
    "            datasets (list): The list of scaled datasets\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        scaler = MinMaxScaler(feature_range=(0,10)) #StandardScaler()\n",
    "        dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "    return datasets\n",
    "\n",
    "scaled_datasets = scale_datasets(numerically_datasets)\n",
    "\n",
    "plot_data_distribution(scaled_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Explore models and pick the best one(s)\n",
    "For each dataset, go through each column and calculate its outliers. Visualize the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer # Imputation transformer for completing missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_dataset_outliers(dataset: pd.DataFrame, dataset_outliers: list):\n",
    "    \"\"\"\n",
    "        This function will plot the dataset and the outliers and display it:\n",
    "        Parameters:\n",
    "            dataset (pandas dataframe): The dataset\n",
    "            dataset_outliers (list): The list of outliers\n",
    "    \"\"\"\n",
    "    \n",
    "    num_cols = 4\n",
    "    num_rows = (len(dataset.columns) // num_cols) + (len(dataset.columns) % num_cols > 0)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, min(4, num_cols), figsize=(15, 5 * num_rows))\n",
    "    axs = axs.flatten() if num_rows * num_cols > 1 else [axs]\n",
    "    \n",
    "    for idx, outliers in enumerate(dataset_outliers):\n",
    "        col = dataset.columns[idx]\n",
    "        # print(f\"Dataset : {dataset.shape} {col}-Outliers: {len(outliers)}\")\n",
    "\n",
    "        axs[idx].plot(dataset[col], alpha=.9, linewidth=.8) # Plot the dataset values\n",
    "        axs[idx].scatter(outliers, dataset[col][outliers], label='Outliers', alpha=.9, color='red', s=10) # Plot the outliers\n",
    "\n",
    "        axs[idx].set_title(f'Dataset-{idx} {dataset.shape}', fontsize=8)\n",
    "        axs[idx].set_xlabel(f'{col.replace(\"$\", \"\")}', fontsize=8)\n",
    "        axs[idx].set_ylabel('Value')\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.5) # Add a space between the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def detect_outliers(datasets: list):\n",
    "    \"\"\"\n",
    "        This function will detect the outliers in the datasets.\n",
    "        \n",
    "        Parameters:\n",
    "            datasets (list): The list of datasets\n",
    "        \n",
    "        Returns:\n",
    "            list: The list of outlier scores\n",
    "    \"\"\"\n",
    "    outlier_scores = []\n",
    "    imputer = SimpleImputer(strategy='mean')  # Impute missing values with the mean\n",
    "    model = IsolationForest(contamination=0.1) # 10% of the data are outliers\n",
    "\n",
    "    for dataset in datasets: # For each dataset in the list of datasets\n",
    "        dataset_outliers = [] # List to store the outliers for each dataset ... necessary?\n",
    "\n",
    "        for col in dataset.columns: # For each column in the current dataset\n",
    "            anomaly_indices = []\n",
    "            \n",
    "            imputer.fit(dataset[[col]]) # Train the imputer on the current column\n",
    "            imputed_values = imputer.transform(dataset[[col]]) # Fill the missing values\n",
    "            if imputed_values.shape[1] == 0: continue\n",
    "            \n",
    "            dataset[col] = pd.DataFrame(imputed_values, columns=[col]) # Replace the column\n",
    "\n",
    "            model.fit(dataset[[col]]) # Fit the model to get baseline\n",
    "            predictions = model.predict(dataset[[col]]) # The anomaly score of the input samples\n",
    "            anomaly_indices = np.where(predictions == -1)[0] # Outliers are labeled -1 and inliers are labeled 1\n",
    "            dataset_outliers.append(anomaly_indices) # Append the indices of the outliers\n",
    "\n",
    "            # print(f'Dataset values and outliers: {dataset.shape} {len(anomaly_indices)} ..{.1*dataset.shape[0]}')\n",
    "        \n",
    "        # Plot anomalies for each column\n",
    "        plot_dataset_outliers(dataset, dataset_outliers)\n",
    "        # break # <- Uncomment to plot all datasets\n",
    "\n",
    "    outlier_scores.append(dataset_outliers)\n",
    "    \n",
    "    return outlier_scores\n",
    "\n",
    "outlier_scores = detect_outliers(scaled_datasets)\n",
    "# outlier_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Present solution\n",
    "\n",
    "Build an app which takes as input _Link_ to a dataset or multiple datasets, and returns a plot of the columns and anomalies in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install protobuf typing_extensions cachetools\n",
    "%pip install altair<6,>=4.0 click<9,>=7.0 gitpython!=3.1.19,<4,>=3.0.7 \n",
    "%pip install pyarrow>=7.0 pydeck<1,>=0.8.0b4 rich<14,>=10.14.0 tenacity<10,>=8.1.0 toml<2,>=0.10.1\n",
    "%pip install pillow==10.0.0\n",
    "%pip install streamlit==1.18.1\n",
    "%pip install streamlit_jupyter\n",
    "\n",
    "%pip freeze > requirements.txt\n",
    "\n",
    "\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from streamlit_jupyter import StreamlitPandas as stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(urls:list):\n",
    "    \"\"\"\n",
    "    Load a dataset .\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the dataset, or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datasets = get_datasets(urls)\n",
    "        datasets = remove_duplicates(datasets)\n",
    "        datasets = remove_small_datasets(datasets)\n",
    "        datasets = handle_missing_values(datasets)\n",
    "        non_alphanumeric_chars = find_non_alphanumeric_data(datasets)\n",
    "        datasets = replace_non_alphanumeric_data(datasets, non_alphanumeric_chars)\n",
    "        datasets = clean_columns(datasets)\n",
    "\n",
    "        numerically_datasets = convert_categorical_columns(datasets)\n",
    "        scaled_datasets = scale_datasets(numerically_datasets)\n",
    "        return scaled_datasets\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load data from {urls}: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detect anomalies in each feature of a DataFrame using Z-score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with feature names as keys and lists of anomaly indices as values.\n",
    "    \"\"\"\n",
    "    outlier_scores = detect_outliers(scaled_datasets)\n",
    "\n",
    "    return outlier_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Anomaly Detection in Datasets\")\n",
    "\n",
    "# Input: List of dataset links\n",
    "dataset_links = st.text_area(\"Enter dataset URLs (one per line):\")\n",
    "if dataset_links:\n",
    "    urls = dataset_links.strip().split(',')\n",
    "    st.subheader(f\"Datasets from input...\")\n",
    "        \n",
    "    # Load the dataset\n",
    "    df = load_dataset(urls)\n",
    "    if df is not None:\n",
    "        st.write(\"Data Preview:\", df.head())\n",
    "        \n",
    "        # Detect anomalies\n",
    "        anomalies = detect_anomalies(df)\n",
    "        st.write('Outliers detected' if len(anomalies) > 0 else 'No outliers detected')\n",
    "stp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_isolation_forest(df):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Isolation Forest.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with feature names as keys and lists of anomaly indices as values.\n",
    "    \"\"\"\n",
    "    anomalies = {}\n",
    "    for column in df.select_dtypes(include=[np.number]).columns:\n",
    "        model = IsolationForest(contamination=0.05)  # Adjust contamination as needed\n",
    "        df['anomaly'] = model.fit_predict(df[[column]])\n",
    "        anomaly_indices = df[df['anomaly'] == -1].index.tolist()  # -1 indicates anomalies\n",
    "        anomalies[column] = anomaly_indices\n",
    "        df.drop('anomaly', axis=1, inplace=True)  # Clean up the DataFrame\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "def main():\n",
    "    st.title(\"Anomaly Detection in Datasets with Isolation Forest\")\n",
    "\n",
    "    dataset_links = st.text_area(\"Enter dataset URLs (one per line):\")\n",
    "    if dataset_links:\n",
    "        urls = dataset_links.strip().split('\\n')\n",
    "        for url in urls:\n",
    "            st.subheader(f\"Dataset from: {url}\")\n",
    "            df = load_dataset(url)\n",
    "            if df is not None:\n",
    "                st.write(\"Data Preview:\", df.head())\n",
    "                anomalies = detect_anomalies_isolation_forest(df)\n",
    "                st.write(\"Anomalies Detected:\")\n",
    "                for feature, indices in anomalies.items():\n",
    "                    st.write(f\"Feature '{feature}': {len(indices)} anomalies at indices {indices}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
